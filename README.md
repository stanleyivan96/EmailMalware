# Email Malware
This repo contains a Jupyter Notebok that predicts the probability of an E-mail containing Malware, using information such as whether the Email contains Exe, Zip, PDF, Doc etc.
The aim of this project is to create a model with improved accuracy, using the bagging tree algorithm, and tune it such that it will be accurate.

# How to view the Jupyter Notebook
Click on the file named 'Detecting Malware from Email.ipynb' to view the Jupyter Notebook. Jupyter Notebooks are automatically rendered by GitHub, so you can view in your browser without installing any applications.

# Approach taken
## Preparing the Data
* To prepare the data for machine learning, the data will need to be in numerical form in order to put in the algorithm. Thankfully, although the data consists mostly of string data, but it was mostly Yes/No and one column consisting of domain suffix data in string format.
Using Python, I mapped all the columns with Yes/No data to 1/0, and used One Hot Encoding to convert the Domain Suffix Data. 

## Using Random Forest Classifier
* I decided to use Random Forest to build the model, as it works similar to a bagging algorithm, which aims to reduce the complexity of models that overfit the training data. 
* From what I have researched, the fundamental difference is that in Random Forests, only a subset of features are selected at random out of the total and the best split feature is used to split each node in a tree, unlike bagging where all features are considered for splitting a node.
* I started out by setting the all the main features as my predictor variable, and 'isMalware' as my response variable.
* After doing a train-test-split, I used the Random Forest Classifier from Scikit-Learn and the initial classification accuracy for train dataset is 0.9954, and 0.9008 for the test dataset.
* Due to the unrealistic value derived from the train dataset, I suspect that there is a overfitting of the model, as the test dataset did not acheived the same levels of accuracy.

## Feature Importance Score
* Before adjusting the parameters for the Random Forest, I went on to find the importance score for the various features, to determine whcih are the features that the most relevant to the target variable.
* This is done in order to improve the efficiency and effectiveness of the predictive model. At the same time, it can help to reduce the issue of overfitting.
* Based on the scores, I decide to only inlcude 4 predictor variables for the next model, which is "totalEmailSizeBytes", "urlCount", "hasUnknown", "hasDoc".
* The new model gives a classification accuracy of 0.9468 for the train dataset, and 0.9112 for the test dataset. 
* This shows that the issue of overfitting has been resolved, and at the same time, the accuracy for the test dataset has increased, which shows that reducing the number of redundant features can reduce the amount of noise to the model, increasing it's accuracy.
* However, the classification accuracy for test dataset can still be further improved. 

## Tuning of parameter using Grid Search CV. 
* The aim now is to increase the classification accuracy for test dataset, and there is a need to tune the various parameters of the Random Forest algorithm.
* I decided to employ the use of Grid Search CV, as it can help to perform hyper parameter tuning in order to determine the optimal values for a given model.
* It is also a effective and efficient way to improve the performance of the Random Forest Model
* Before using Grid Search CV, I will first need to consider what are the parameters that need to be varied, and for Random Forest, the two key parameters are "max_depth" & "max_features".
* To reduce the overfitting, we would want to check what is the optimal number of trees to be used in the forest, as random forest is an ensemble method comprising of creating multiple decision trees. We would also need to find out the max number of features considered when finding the best split to get a true random forest.
* Taking the parameter values from the GridSearchCV, I created another model and it gives a classification accuracy of 0.9349 for the train dataset, and 0.9324 for the test dataset.
* This is a significant improvement compared to the previous model, and the range of values for both are similar as well, hence the problem of overfitting is not so severe anymore

## Output of prediction data
* Using the optimized model derived previously, the prediction data can now be generated as a .csv data.
